
"""
å®åä¸¾æŠ¥äº‹ä»¶èˆ†è®ºåˆ†æï¼ˆçº¯æ–‡å­—ç‰ˆï¼‰
--------------------------------------
åŠŸèƒ½ï¼š
1. åˆ†æäº‹ä»¶çƒ­åº¦ã€äº’åŠ¨ã€åœ°åŒºåˆ†å¸ƒã€æ—¶é—´è¶‹åŠ¿
2. è¾“å‡ºç®€æ´æ–‡å­—æŠ¥å‘Šï¼ˆæ§åˆ¶å°æ‰“å° + æ–‡ä»¶ä¿å­˜ï¼‰
è¿è¡Œæ–¹å¼ï¼š
    python public_opinion_analysis_text.py
"""

import json
import pandas as pd
from pathlib import Path
from collections import Counter
from datetime import datetime


# =============================
# æ•°æ®åŠ è½½ä¸åŸºç¡€è®¡ç®—
# =============================

def load_event_data(json_path):
    """åŠ è½½äº‹ä»¶èšç±»ç»“æœ"""
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data["events"]

def compute_event_metrics(event):
    """è®¡ç®—å•ä¸ªäº‹ä»¶çš„èˆ†è®ºæŒ‡æ ‡"""
    stats = event.get("interaction_statistics", {})
    region = event.get("region", "æœªçŸ¥")
    rep_post = event.get("representative_post_clean", "")[:60]
    duration = event.get("time_span_analysis", {}).get("duration_days", 0)
    tinfo = event.get("time_span_analysis", {})

    total_heat = (
        stats.get("total_likes", 0)
        + 2 * stats.get("total_comments", 0)
        + 3 * stats.get("total_shares", 0)
        + stats.get("total_favorites", 0)
    )
    avg_heat = (
        stats.get("avg_likes", 0)
        + 2 * stats.get("avg_comments", 0)
        + 3 * stats.get("avg_shares", 0)
        + stats.get("avg_favorites", 0)
    )

    return {
        "event_id": event.get("event_id"),
        "region": region,
        "ä»£è¡¨å¸–": rep_post,
        "å¸–å­æ•°": event.get("post_count", 0),
        "å¹³å‡ç‚¹èµ": stats.get("avg_likes", 0),
        "å¹³å‡è¯„è®º": stats.get("avg_comments", 0),
        "å¹³å‡è½¬å‘": stats.get("avg_shares", 0),
        "å¹³å‡æ”¶è—": stats.get("avg_favorites", 0),
        "æ€»çƒ­åº¦æŒ‡æ•°": total_heat,
        "å¹³å‡çƒ­åº¦æŒ‡æ•°": avg_heat,
        "ä¼ æ’­æ—¶é•¿(å¤©)": duration,
        "å¼€å§‹æ—¶é—´": tinfo.get("start_time"),
        "ç»“æŸæ—¶é—´": tinfo.get("end_time"),
    }

def analyze_events(events):
    """ç”Ÿæˆå®Œæ•´åˆ†æè¡¨"""
    metrics = [compute_event_metrics(e) for e in events]
    df = pd.DataFrame(metrics)
    df = df.sort_values(by="æ€»çƒ­åº¦æŒ‡æ•°", ascending=False).reset_index(drop=True)
    return df


# =============================
# æ–‡æœ¬åˆ†æä¸æŠ¥å‘Šç”Ÿæˆ
# =============================

def summarize_analysis(df):
    """ç”Ÿæˆæ–‡å­—åˆ†ææŠ¥å‘Š"""

    lines = []
    lines.append("ğŸ“Š å®åä¸¾æŠ¥äº‹ä»¶èˆ†è®ºåˆ†ææŠ¥å‘Š")
    lines.append("=" * 50)

    # æ€»ä½“æ¦‚å†µ
    lines.append(f"\nå…±ç»Ÿè®¡äº‹ä»¶æ•°é‡: {len(df)}")
    lines.append(f"å¹³å‡å¸–å­æ•°: {df['å¸–å­æ•°'].mean():.1f}")
    lines.append(f"å¹³å‡çƒ­åº¦æŒ‡æ•°: {df['æ€»çƒ­åº¦æŒ‡æ•°'].mean():.1f}")
    lines.append(f"å¹³å‡ä¼ æ’­æ—¶é•¿: {df['ä¼ æ’­æ—¶é•¿(å¤©)'].mean():.1f} å¤©")

    # çƒ­åº¦å‰äº”äº‹ä»¶
    lines.append("\nğŸ”¥ çƒ­åº¦æœ€é«˜çš„ 5 ä¸ªäº‹ä»¶ï¼š")
    for i, row in df.head(5).iterrows():
        lines.append(
            f"{i+1}. {row['ä»£è¡¨å¸–']} | åœ°åŒº: {row['region']} | çƒ­åº¦æŒ‡æ•°: {row['æ€»çƒ­åº¦æŒ‡æ•°']:,} | "
            f"å¹³å‡ç‚¹èµ: {row['å¹³å‡ç‚¹èµ']:.0f} | è½¬å‘: {row['å¹³å‡è½¬å‘']:.0f} | è¯„è®º: {row['å¹³å‡è¯„è®º']:.0f}"
        )

    # åœ°åŒºåˆ†å¸ƒ
    region_counts = df["region"].value_counts()
    lines.append("\nğŸŒ ä¸¾æŠ¥äº‹ä»¶åœ°åŒºåˆ†å¸ƒï¼š")
    for region, count in region_counts.items():
        lines.append(f" - {region}: {count} èµ· ({count/len(df)*100:.1f}%)")

    # æ—¶é—´è¶‹åŠ¿åˆ†æ
    df["å¼€å§‹æ—¶é—´_dt"] = pd.to_datetime(df["å¼€å§‹æ—¶é—´"], errors="coerce")
    time_df = df.dropna(subset=["å¼€å§‹æ—¶é—´_dt"]).sort_values("å¼€å§‹æ—¶é—´_dt")

    if not time_df.empty:
        first = time_df["å¼€å§‹æ—¶é—´_dt"].iloc[0]
        last = time_df["å¼€å§‹æ—¶é—´_dt"].iloc[-1]
        lines.append(f"\nâ° æ•°æ®æ—¶é—´èŒƒå›´ï¼š{first.date()} ~ {last.date()}")
        # çƒ­åº¦éšæ—¶é—´å˜åŒ–è¶‹åŠ¿
        early = time_df.head(len(time_df)//3)["æ€»çƒ­åº¦æŒ‡æ•°"].mean()
        mid = time_df.iloc[len(time_df)//3: 2*len(time_df)//3]["æ€»çƒ­åº¦æŒ‡æ•°"].mean()
        late = time_df.tail(len(time_df)//3)["æ€»çƒ­åº¦æŒ‡æ•°"].mean()
        trend = "ä¸Šå‡" if late > early else "ä¸‹é™" if late < early else "æŒå¹³"
        lines.append(f"æ€»ä½“çƒ­åº¦è¶‹åŠ¿ï¼š{trend}ï¼ˆæ—©æœŸå‡å€¼={early:.0f}, åæœŸå‡å€¼={late:.0f}ï¼‰")

    # è¾“å‡ºæ‘˜è¦
    report = "\n".join(lines)
    return report


# =============================
# æ ‡ç­¾ç»Ÿè®¡
# =============================

def analyze_tags(events, top_n=15):
    """ç»Ÿè®¡é«˜é¢‘æ ‡ç­¾"""
    all_tags = []
    for e in events:
        for p in e.get("all_posts", []):
            tags = p.get("tags")
            if not tags:
                continue
            if isinstance(tags, str):
                try:
                    tags_list = eval(tags)
                except Exception:
                    tags_list = [tags]
            else:
                tags_list = tags
            all_tags.extend(tags_list)
    counter = Counter(all_tags)
    return counter.most_common(top_n)


# =============================
# ä¸»ç¨‹åº
# =============================

def main(json_path="/data2/jrjiang/realname/data/events_result_final_robust.json"):
    print("ğŸš€ æ­£åœ¨åŠ è½½æ•°æ®å¹¶ç”Ÿæˆèˆ†è®ºåˆ†ææŠ¥å‘Š...")
    events = load_event_data(json_path)
    df = analyze_events(events)

    # æ–‡å­—æŠ¥å‘Š
    report = summarize_analysis(df)

    # é«˜é¢‘æ ‡ç­¾
    tags = analyze_tags(events)
    tag_text = "\n\n#ï¸âƒ£ é«˜é¢‘æ ‡ç­¾ï¼š\n" + "\n".join([f"{i+1}. {t[0]} ({t[1]} æ¬¡)" for i, t in enumerate(tags)])
    report += tag_text

    # ä¿å­˜åˆ°æ–‡ä»¶
    out_path = Path("/data2/jrjiang/realname/code/event_detection/output/èˆ†è®ºåˆ†ææŠ¥å‘Š.txt")
    out_path.write_text(report, encoding="utf-8")

    print("\nâœ… åˆ†æå®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜ä¸ºï¼šèˆ†è®ºåˆ†ææŠ¥å‘Š.txt")
    print("ğŸ“„ æŠ¥å‘Šæ‘˜è¦é¢„è§ˆï¼š\n")
    print("\n".join(report.splitlines()[:25]))  # åªé¢„è§ˆå‰ 25 è¡Œ


if __name__ == "__main__":
    main()
